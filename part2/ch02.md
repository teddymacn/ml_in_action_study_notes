# How the backpropagation algorithm works

The backpropagation algorithm is the essence of modern neural networks algorithms, it works far faster than earlier approaches for a neural network to learn, making it possible to use neural networks to solve problems which had previously been insoluble.

The idea of backpropagation actually inherits from the idea of gradient descent based weights & bias learning in logistic classification, which we have already been familar with. The difference is each neuron links to the neurons in its next layer, so we can only know the cost of each neuron if we have already known the cost of the linked neurons in the next layer. And since neural networks are still supervised learning, we do already know the cost of the output layer neunons (actual ouput - predict output). So we could backward compute the cost of each neuron layer by layer. And as long as we know the cost of each neuron, to learn the weights and bias of each neuron has no difference from in logistic classification.

## Common Cost Functions

Similar to gradient descent in logistic classification, the effect of the learning process of backpropagation depends on the activation function and cost function chosen. We have summarized the common activation functions in [chapter 1](./ch01.md), so here let me summarize some common cost functions. There are more other cost functions which might work better for specific cases. But the following ones are the most common ones.

- Mean Squared Error (MSE), or quadratic
- Cross-entropy
- Negative Logarithmic Likelihood

Mean Squared Error (MSE), or quadratic function is widely used in linear regression, its basic idea is that the optimized fitting line should be a line which minimizes the sum of distance of each point to the regression line. However, for logistic classification and neural networks, if using sigmoid as the activation function, the quadratic cost function would suffer the problem of slow convergence. So try to use other activation functions such as ReLu together with it.

Cross Entropy is commonly-used in logistic classification and neural networks, it measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. Cross entropy does not have slow convergence problem when working with sigmoid as the activation function.

Negative Log Likelihood cost function is also known as the multiclass cross-entropy. It is widely used in neural networks, and is used when the model outputs a probability for each class, rather than just the most likely class.