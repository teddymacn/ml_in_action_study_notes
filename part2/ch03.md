# Improving the way neural networks learn

In [previous chapter](./ch02.md), it mainly talked about how backpropagation works and we know that some combinations of activation function and cost function (sigmoid + quadratic) might cause slow convergence problem.

In this chapter, it will try to improve the simple neural network we built in [chapter 1](./ch01.md) from several different aspects.

First, we will use cross entropy cost function together with sigmoid to avoid the slow convergence problem.

Now our neural networks could converge fast as expected, but we might soon meet another common problem as in training a regression model - overfitting.

So second, we will introduce a new concept "regularization" which could help to solve the overfitting problem. The regularization we will use in the new demo code is L2 regularization, which is the most commonly used one in neural networks. 

Third, there is one more place we could adjust to give impact the training process of neural networks, which is the weights & biases initialization. In the demo of [chapter 1](./ch01.md), we simply used Gaussian random variables. And in the demo if this chapter, we will use normalized Gaussians, specifically, Gaussian random variables over the square root of the number of the weights connecting to the same neuron, which has been proved to speed up the learning process.

Four, in the demo of [chapter 1](./ch01.md), when training, we specify the epochs, and the training will stop only when it finishes all the epochs. But we know that in late epochs, the accuracy get improved very slow till kind of saturated. So the last enhancement we will do in the demo of this chapter is to support early stopping, once the classification accuracy on the validation_data has saturated, we stop training.

## Common Regularization Methods

- L2 regularization
- L1 regularization
- Dropout
- Artificial expansion of the training data

The idea of L2 regularization is to add an extra term to the cost function, a term called the regularization term which is the sum of the squares of all the weights in the network and is scaled by a factor 位/2n, where 位>0 is known as the regularization parameter, and n is, as usual, the size of our training set.

In L1 regularization we modify the unregularized cost function by adding the sum of the absolute values of the weights and is scaled by a factor 位/n, the definition of 位 and n are exactly same as in L2 regularization.

In L1 regularization, the weights shrink by a constant amount toward 0. In L2 regularization, the weights shrink by an amount which is proportional to w. And so when a particular weight has a large magnitude, |w|, L1 regularization shrinks the weight much less than L2 regularization does.

Dropout is a radically different technique for regularization. Unlike L1 and L2 regularization, dropout doesn't rely on modifying the cost function. Instead, in dropout we modify the network itself, specifically, we remove neurons from the hidden layers during the training. 

When we dropout different sets of neurons, it's rather like we're training different neural networks. And so the dropout procedure is like averaging the effects of a very large number of different networks. The different networks will overfit in different ways, and so, hopefully, the net effect of dropout will be to reduce overfitting. Of course, the true measure of dropout is that it has been very successful in improving the performance of neural networks. 

The last method mentioned in this chapter about regularization is the artificial expansion of the training data. It is easy to imagine that when we have small training dataset, it is more often to overfit. To get a large training dataset might be expensive, but how about if we could expand the existing small training dataset to be a larger one? Specifically, for the mnist scenario, suppose we only have 1000 traning examples, which is not big. We could try to modify each existing training example image by adding some noise to it, or to rotate the image a little bit, etc, so that we get a much larger training dataset, and the overfitting problem could be relieved a lot.

## Demo Code

[network2.py](network2.py) - The improved neural network implementation.