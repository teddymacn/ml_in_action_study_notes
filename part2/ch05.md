# Why are deep neural networks hard to train?

I skipped chapter 4 coz it mainly trys to proof that neural networks can compute any function. We could simply believe it.

But this chapter 5 is important. We finally begin to talk about deep learning. Comparing to shallow neural networks, which only has one hidden layer, deep neural networks might have much more hidden layers.

But new problems will come when our neural networks get deeper.

# The vanishing gradient problem

The first new problem comes is the vanishing gradient problem. In short, it means that the farer the layers are from the output layer, the slower the weights and biases learn. And it exponentially gets slower.

In [chapter 1](./ch01.md) when talking about common activation functions. I already mentioned the vanishing gradient problem of sigmoid and tanh, and the ReLu activation function don't have this issue. That's why ReLu and its variants are popular in deep neural networks. And also when using ReLu, you also need to use Softmax in the output layer to convert the output values to 0 or 1 for classification.

# The heavier computation cost problem

The second problem comes, although not mentioned in this chapter explicitly is the deeper a neural network is, the heavier the computation it needs. And modern CPUs are not quite friendly to matrix computationn. To solve this problem, we need to be able to execute our matrix computation in GPU, which will be 20x times faster than CPU on my local machine. So in the deep learning demo of next chapter, we will use the theano library to benefit from GPU for computation.