# Using neural nets to recognize handwritten digits

The first chapter of this book begins from the classic real-life handwriting digits classification problem, which is simple enough but really a challenage for most traditional machine learning algoithms we discussed in part 1. Of course, it doesn't mean traditional machine learning algorithms could do nothing for it. For example, in [part 1 ch02](../../part1/ch02/README.md) and [part 1 ch06](../../part1/ch06/README.md) we used the kNN and SVM algorithms did the same job. But you should realize, kNN is not efficent at all coz it needs to compare the distance of each example in the training dataset when classifying each test example. When you have a big training dataset, the performance of kNN will not be accetable. 

SVM is more comparable to neural networks for the handwriting scenario, especially for a dataset size like mnist. The [MNIST benchmarks](http://yann.lecun.com/exdb/mnist/) shows: The best-performing model in this set is a committee consisting of 35 ConvNets, which were reported to have a 0.23% test error; the best SVM model has a test error of 0.56%. But without a question, we'd say that the 35 ConvNet committee is far more computationally expensive. So, if you make that decision: Is a 0.33% improvement worth it? In some cases, it's maybe worth it (e.g., in the financial sector for non-real time predictions), in other cases it perhaps won't be worth it, though. But SVM is not as easy as neural networks for distributed computing, when we are talking about a huge training set, neural network is much more friendly to map-reduce, so we could train a neural network with a distributed computing platform, such as tensorflow, much more easily then SVM.

## The architecture of neural networks

A common neural network consists of layers of neurons. One input layer, one output layer and at least one hidden layer. The model of neurons comes from the model of perceptrons with an additional activation function applying to the output of a perceptron. A perceptron is a model simply accepts multiple weighted input values, and its output value is the linear combination of all the input values multipling their weights plus a bias value.

Why we need the activation functions is because, we want that a small change in the weights of any inputs causing a small corresponding change in the output of each neuron. And this is the core maths why a neural network could be trained.

Common activation functions:

- Sigmoid - Logistic
- Tanh - Hyperbolic tangent
- ReLu - Rectified linear units

Sigmoid is quite popular in shallow neural networks, which works well for most cases. But first it has serious vanishing gradient problem when neural networks get deeper (having more hidden layers). And second, because sigmoid outputs range (0, 1), which makes the gradient updates go too far in different directions which causes slow convergence.

Tanh has a similar shape comparing to sigmoid, but it outputs range (-1, 1), so it doesn't have the slow convergence issue as sigmoid. But because of the similar shape, it also has the vanishing gradient problem.

ReLu becomes very popular in the past couple of years. It was recently proved that it had 6 times improvement in convergence from Tanh function. And it also avoids and rectifies vanishing gradient problem. So almost all deep learning models use ReLu nowadays. Its main limitation is: It should only be used within hidden layers. And for output layers we should use a softmax function for classification problems to compute the probabilites for the classes; and for regression problems we should use a linear function.

There is a potential problem of ReLu: It is possible (although not often) that some gradients can be fragile during training and might even die, meaning it can cause a weight update which will makes it never activate on any data point again. But there are variants of ReLu such as leaky ReLu or Maxout which could avoid the dead neurons pronlem.

## A simple network to classify handwritten digits

The entire book demonstrates how to use neural networks to do the mnist handwriting classification better and better. In this first chapter, it starts from a most common neural network architecture without any optimizations on weights, activation functions, cost functions or regularizations, etc. We can see that even without much optimizations, a the neural network could already do the job quite well.

And we could get the basic feeling about how a neural network works in feedforward, backpropagation and how weights and bias values are trained and adjusted via stochastic gradient descent, and so that the neural network finally is able to classify a digit in more than 95% acuracy.

To start from the simplest model not only make the neural network easy for us to understand, but also demonstrates the status of the early age when the neural network tool was just invented. In later chapters, we will try to enhance our neural networks from different aspects to get better classification acuracy just like what the early age researchers did before.

## Demo Code

[network.py](network.py) - The simple neural network implementation.

[mnist_svm.py](mnist_svm.py) - A scikit-learn based simple SVM implementation (performance not good, it took about 10 mins to run on my i7 CPU, but result not bad, 94.35% acuracy).
